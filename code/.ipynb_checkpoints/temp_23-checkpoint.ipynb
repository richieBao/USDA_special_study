{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8500d-daa8-42fc-bc10-25669da2836d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97a958b-619d-4dc5-b882-e5edcf2c0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_tag_v3\n",
    "env = simple_tag_v3.env(render_mode='human')\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_space(agent).sample() # this is where you would insert your policy\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa30f2b1-b8dd-4546-9fb3-7f222ed85f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseWrapper.observation_space of <pettingzoo.utils.wrappers.order_enforcing.OrderEnforcingWrapper object at 0x000002BDE34CA4D0>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f627b7-0eb9-41e6-ac53-317589962509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import Collector\n",
    "from tianshou.env import DummyVectorEnv, PettingZooEnv\n",
    "from tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n",
    "\n",
    "from pettingzoo.classic import rps_v2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load the PettingZoo environment\n",
    "    env = rps_v2.env(render_mode=\"human\")\n",
    "\n",
    "    # Step 2: Wrap the environment for Tianshou interfacing\n",
    "    env = PettingZooEnv(env)\n",
    "\n",
    "    # Step 3: Define policies for each agent\n",
    "    policies = MultiAgentPolicyManager([RandomPolicy(), RandomPolicy()], env)\n",
    "\n",
    "    # Step 4: Convert the env to vector format\n",
    "    env = DummyVectorEnv([lambda: env])\n",
    "\n",
    "    # Step 5: Construct the Collector, which interfaces the policies with the vectorised environment\n",
    "    collector = Collector(policies, env)\n",
    "\n",
    "    # Step 6: Execute the environment with the agents playing for 1 episode, and render a frame every 0.1 seconds\n",
    "    result = collector.collect(n_episode=10, render=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d1801-ddc2-4175-b382-0e2d63f4c644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23d909b-91e4-4293-9253-98cc07413e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09929343-733b-4285-8905-60b33f8590cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83c46cc1-00a6-4d45-b0d9-ea128e8929aa",
   "metadata": {},
   "source": [
    "#### 2）算法集成\n",
    "\n",
    "| 分类  | 序号  | 算法  | 论文  | 备注  |\n",
    "|---|---|---|---|---|\n",
    "| 基于策略梯度的深度强化学习算法  |  1 | PG，Policy Gradient（策略梯度）   |  Policy gradient methods for reinforcement learning with function approximation<sup>[Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999], 1057–1063. 1999. URL: http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.]</sup> |   |\n",
    "|   | 2  | A2C，Advantage Actor-Critic（优势动作评价）  | Asynchronous Methods for Deep Reinforcement Learning<sup>[Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, 1928–1937. 2016. URL: http://proceedings.mlr.press/v48/mniha16.html.]</sup>  |   |\n",
    "|   | 3  | PPO，Proximal Policy Optimization（近端策略优化）  | Proximal policy optimization algorithms<sup>[John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, 2017. URL: http://arxiv.org/abs/1707.06347, arXiv:1707.06347.]</sup>   |   |\n",
    "|   | 4  | GAE，Generalized Advantage Estimator（广义优势函数估计器）  | High-dimensional continuous control using generalized advantage estimation<sup>[John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. 2016. URL: http://arxiv.org/abs/1506.02438.]</sup>  |   |\n",
    "|   | 5  |PPG，Phasic Policy Gradient   | Phasic Policy Gradient<sup>[Karl Cobbe, Jacob Hilton, Oleg Klimov, & John Schulman. (2020). Phasic Policy Gradient.]</sup>  |   |\n",
    "| 基于Q价值函数的深度强化学习算法  | 6  | DQN，Deep Q Network（深度Q网络）  |  Human-level control through deep reinforcement learning<sup>[Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. URL: https://doi.org/10.1038/nature14236, doi:10.1038/nature14236.]</sup>  |   |\n",
    "|   | 7  | DDQN，Double DQN（双网络深度Q学习）  | Deep reinforcement learning with double q-learning<sup>[Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, 2094–2100. 2016. URL: http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389.]</sup>  |   |\n",
    "|   |  8 | PER，Prioritized Experience Replay（优先级经验重放）  | Prioritized experience replay<sup>[Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. 2016. URL: http://arxiv.org/abs/1511.05952.]</sup>  |   |\n",
    "|   | 9  | C51，Categorical DQN  |  A Distributional Perspective on Reinforcement Learning<sup>[Marc G. Bellemare, Will Dabney, & Rémi Munos. (2017). A Distributional Perspective on Reinforcement Learning.]</sup> |   |\n",
    "| 综合Q价值函数与策略梯度的深度强化学习算法  |  10 | DDPG，Deep Deterministic Policy Gradient（深度确定性策略梯度）  | Continuous control with deep reinforcement learning<sup>[Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. 2016. URL: http://arxiv.org/abs/1509.02971.]</sup>  |   |\n",
    "|   | 11  | TD3，Twin Delayed DDPG（双延迟深度确定性策略梯度）  | Addressing function approximation error in actor-critic methods<sup>[Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, 1582–1591. 2018. URL: http://proceedings.mlr.press/v80/fujimoto18a.html.]</sup>  |   |\n",
    "|   | 12  | SAC，Soft Actor-Critic（软动作评价）  | Soft actor-critic algorithms and applications<sup>[Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms and applications. CoRR, 2018. URL: http://arxiv.org/abs/1812.05905, arXiv:1812.05905.]</sup>  |   |\n",
    "|  其它 | 13  | RND，Random Network Distillation  | Exploration by Random Network Distillation<sup>[Yuri Burda, Harrison Edwards, Amos Storkey, & Oleg Klimov. (2018). Exploration by Random Network Distillation.]</sup>  |   |\n",
    "|   |  14 |   Qdagger |  Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress<sup>[Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, & Marc G. Bellemare. (2022). Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress.]</sup> |   |\n",
    "|   |   |   |   |   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
